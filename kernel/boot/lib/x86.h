#ifndef _CERTIKOS_BOOT_X86_H_
#define _CERTIKOS_BOOT_X86_H_

#include <types.h>
#include <gcc.h>

// EFLAGS register
#define FL_CF		0x00000001	// Carry Flag
#define FL_PF		0x00000004	// Parity Flag
#define FL_AF		0x00000010	// Auxiliary carry Flag
#define FL_ZF		0x00000040	// Zero Flag
#define FL_SF		0x00000080	// Sign Flag
#define FL_TF		0x00000100	// Trap Flag
#define FL_IF		0x00000200	// Interrupt Flag
#define FL_DF		0x00000400	// Direction Flag
#define FL_OF		0x00000800	// Overflow Flag
#define FL_IOPL_MASK	0x00003000	// I/O Privilege Level bitmask
#define FL_IOPL_0	0x00000000	//   IOPL == 0
#define FL_IOPL_1	0x00001000	//   IOPL == 1
#define FL_IOPL_2	0x00002000	//   IOPL == 2
#define FL_IOPL_3	0x00003000	//   IOPL == 3
#define FL_NT		0x00004000	// Nested Task
#define FL_RF		0x00010000	// Resume Flag
#define FL_VM		0x00020000	// Virtual 8086 mode
#define FL_AC		0x00040000	// Alignment Check
#define FL_VIF		0x00080000	// Virtual Interrupt Flag
#define FL_VIP		0x00100000	// Virtual Interrupt Pending
#define FL_ID		0x00200000	// ID flag

// Trap numbers
// These are processor defined:
#define T_DIVIDE     0		// divide error
#define T_DEBUG      1		// debug exception
#define T_NMI        2		// non-maskable interrupt
#define T_BRKPT      3		// breakpoint
#define T_OFLOW      4		// overflow
#define T_BOUND      5		// bounds check
#define T_ILLOP      6		// illegal opcode
#define T_DEVICE     7		// device not available
#define T_DBLFLT     8		// double fault
#define T_COPROC	 9		// reserved (not generated by recent processors)
#define T_TSS       10		// invalid task switch segment
#define T_SEGNP     11		// segment not present
#define T_STACK     12		// stack exception
#define T_GPFLT     13		// general protection fault
#define T_PGFLT     14		// page fault
#define T_RES		15		// reserved
#define T_FPERR     16		// floating point error
#define T_ALIGN     17		// aligment check
#define T_MCHK      18		// machine check
#define T_SIMDERR   19		// SIMD floating point error

#define T_IRQ0		32	// This trap corresponds to IRQ0 (once it is remapped).

// We use these vectors to receive local per-CPU interrupts
#define T_LTIMER        49      // Local APIC timer interrupt
#define T_LERROR        50      // Local APIC error interrupt
#define T_PERFCTR       51      // Performance counter overflow interrupt

#define T_DEFAULT       500     // Unused trap vectors produce this value
#define T_ICNT          501     // Child process instruction count expired

// Hardware IRQ numbers. We receive these as (T_IRQ0 + IRQ_WHATEVER)
#define IRQ_TIMER        0
#define IRQ_KBD          1
#define IRQ_SERIAL       4
#define IRQ_SPURIOUS     7
#define IRQ_IDE         14
#define IRQ_ERROR       19


/* [REF] AMD64 manual vol. 2, p. 59 */
#define _X86_CR4_PSE 4 /* Page Size Extensions */
#define _X86_CR4_PAE 5 /* Physical-Address Extension */
#define _X86_CR4_PGE 7 /* Page-Global Enable */
#ifdef __ASSEMBLY__
#  define X86_CR4_PSE  ( 1 << _X86_CR4_PSE )
#  define X86_CR4_PAE  ( 1 << _X86_CR4_PAE )
#  define X86_CR4_PGE  ( 1 << _X86_CR4_PGE )
#else /* ! __ASSEMBLY__ */
#  define X86_CR4_PSE  ( 1UL << _X86_CR4_PSE )
#  define X86_CR4_PAE  ( 1UL << _X86_CR4_PAE )
#  define X86_CR4_PGE  ( 1UL << _X86_CR4_PGE )
#endif /* __ASSEMBLY__ */

/* [REF] AMD64 manual vol. 2, p. 53 */
#define _X86_CR0_PE  0 /* Protection enabled */
#define _X86_CR0_MP  1 /* Monitor coprocessor */
#define _X86_CR0_ET  4 /* Extension type */
#define _X86_CR0_NE  5 /* Numeric error */
#define _X86_CR0_WP 16 /* Write protected */
#define _X86_CR0_AM 18 /* Aligned mask */
#define _X86_CR0_NW 29 /* Not Writethrough */
#define _X86_CR0_CD 30 /* Cache Disable */
#define _X86_CR0_PG 31 /* Paging */
#ifdef __ASSEMBLY__
#  define X86_CR0_PE  ( 1 << _X86_CR0_PE )
#  define X86_CR0_MP  ( 1 << _X86_CR0_MP )
#  define X86_CR0_ET  ( 1 << _X86_CR0_ET )
#  define X86_CR0_NE  ( 1 << _X86_CR0_NE )
#  define X86_CR0_WP  ( 1 << _X86_CR0_WP )
#  define X86_CR0_AM  ( 1 << _X86_CR0_AM )
#  define X86_CR0_NW  ( 1 << _X86_CR0_NW )
#  define X86_CR0_CD  ( 1 << _X86_CR0_CD )
#  define X86_CR0_PG  ( 1 << _X86_CR0_PG )
#else /* ! __ASSEMBLY__ */
#  define X86_CR0_PE  ( 1UL << _X86_CR0_PE )
#  define X86_CR0_MP  ( 1UL << _X86_CR0_MP )
#  define X86_CR0_ET  ( 1UL << _X86_CR0_ET )
#  define X86_CR0_NE  ( 1UL << _X86_CR0_NE )
#  define X86_CR0_WP  ( 1UL << _X86_CR0_WP )
#  define X86_CR0_AM  ( 1UL << _X86_CR0_AM )
#  define X86_CR0_NW  ( 1UL << _X86_CR0_NW )
#  define X86_CR0_CD  ( 1UL << _X86_CR0_CD )
#  define X86_CR0_PG  ( 1UL << _X86_CR0_PG )

#  define X86_RFLAGS_TF         ( 1UL << 8 )
#  define X86_DR6_BS            ( 1UL << 14 )
#endif /* __ASSEMBLY__ */

/* MSR name and MSR address (AMD64 manual vol. 2, pp. 506-509) */
#define MSR_PAT            0x00000277 /* Page-attribute table (PAT) */
#define MSR_K7_HWCR        0xc0010015
#define MSR_K8_VM_HSAVE_PA 0xc0010117
#define MSR_EFER           0xc0000080 /* Extended feature register */


/* EFER bits (vol. 2, p. 55) */
#define _EFER_SCE   0 /* System Call Extensions */
#define _EFER_LME   8 /* Long mode enable */
#define _EFER_LMA  10 /* Long mode active */
#define _EFER_NX   11 /* No execute enable */
#define _EFER_SVME 12 /* Secure Virtual Machine Enable */
#ifdef __ASSEMBLY__
#  define EFER_SCE   ( 1 << _EFER_SCE )
#  define EFER_LME   ( 1 << _EFER_LME )
#  define EFER_LMA   ( 1 << _EFER_LMA )
#  define EFER_NX    ( 1 << _EFER_NX )
#  define EFER_SVME  ( 1 << _EFER_SVME )
#else /* ! __ASSEMBLY__ */
#  define EFER_SCE   ( 1UL << _EFER_SCE )
#  define EFER_LME   ( 1UL << _EFER_LME )
#  define EFER_LMA   ( 1UL << _EFER_LMA )
#  define EFER_NX    ( 1UL << _EFER_NX )
#  define EFER_SVME  ( 1UL << _EFER_SVME )
#endif /* __ASSEMBLY__ */

#define rdmsr(msr,val1,val2) \
       __asm__ __volatile__("rdmsr" \
			    : "=a" (val1), "=d" (val2) \
			    : "c" (msr))

#define wrmsr(msr,val1,val2) \
     __asm__ __volatile__("wrmsr" \
			  : /* no outputs */ \
			  : "c" (msr), "a" (val1), "d" (val2))


#ifndef __ASSEMBLER__

static gcc_inline void
breakpoint(void)
{
	__asm __volatile("int3");
}

static gcc_inline uint8_t
inb(int port)
{
	uint8_t data;
	__asm __volatile("inb %w1,%0" : "=a" (data) : "d" (port));
	return data;
}

static gcc_inline void
insb(int port, void *addr, int cnt)
{
	__asm __volatile("cld\n\trepne\n\tinsb"			:
			 "=D" (addr), "=c" (cnt)		:
			 "d" (port), "0" (addr), "1" (cnt)	:
			 "memory", "cc");
}

static gcc_inline uint16_t
inw(int port)
{
	uint16_t data;
	__asm __volatile("inw %w1,%0" : "=a" (data) : "d" (port));
	return data;
}

static gcc_inline void
insw(int port, void *addr, int cnt)
{
	__asm __volatile("cld\n\trepne\n\tinsw"			:
			 "=D" (addr), "=c" (cnt)		:
			 "d" (port), "0" (addr), "1" (cnt)	:
			 "memory", "cc");
}

static gcc_inline uint32_t
inl(int port)
{
	uint32_t data;
	__asm __volatile("inl %w1,%0" : "=a" (data) : "d" (port));
	return data;
}

static gcc_inline void
insl(int port, void *addr, int cnt)
{
	__asm __volatile("cld\n\trepne\n\tinsl"			:
			 "=D" (addr), "=c" (cnt)		:
			 "d" (port), "0" (addr), "1" (cnt)	:
			 "memory", "cc");
}

static gcc_inline void
outb(int port, uint8_t data)
{
	__asm __volatile("outb %0,%w1" : : "a" (data), "d" (port));
}

static gcc_inline void
outsb(int port, const void *addr, int cnt)
{
	__asm __volatile("cld\n\trepne\n\toutsb"		:
			 "=S" (addr), "=c" (cnt)		:
			 "d" (port), "0" (addr), "1" (cnt)	:
			 "cc");
}

static gcc_inline void
outw(int port, uint16_t data)
{
	__asm __volatile("outw %0,%w1" : : "a" (data), "d" (port));
}

static gcc_inline void
outsw(int port, const void *addr, int cnt)
{
	__asm __volatile("cld\n\trepne\n\toutsw"		:
			 "=S" (addr), "=c" (cnt)		:
			 "d" (port), "0" (addr), "1" (cnt)	:
			 "cc");
}

static gcc_inline void
outsl(int port, const void *addr, int cnt)
{
	__asm __volatile("cld\n\trepne\n\toutsl"		:
			 "=S" (addr), "=c" (cnt)		:
			 "d" (port), "0" (addr), "1" (cnt)	:
			 "cc");
}

static gcc_inline void
outl(int port, uint32_t data)
{
	__asm __volatile("outl %0,%w1" : : "a" (data), "d" (port));
}

static gcc_inline void
invlpg(void *addr)
{
	__asm __volatile("invlpg (%0)" : : "r" (addr) : "memory");
}

static gcc_inline void
lidt(void *p)
{
	__asm __volatile("lidt (%0)" : : "r" (p));
}

static gcc_inline void
lldt(uint16_t sel)
{
	__asm __volatile("lldt %0" : : "r" (sel));
}

static gcc_inline void
ltr(uint16_t sel)
{
	__asm __volatile("ltr %0" : : "r" (sel));
}

static gcc_inline void
lcr0(uint32_t val)
{
	__asm __volatile("movl %0,%%cr0" : : "r" (val));
}

static gcc_inline uint32_t
rcr0(void)
{
	uint32_t val;
	__asm __volatile("movl %%cr0,%0" : "=r" (val));
	return val;
}

static gcc_inline uint32_t
rcr2(void)
{
	uint32_t val;
	__asm __volatile("movl %%cr2,%0" : "=r" (val));
	return val;
}

static gcc_inline void
lcr3(uint32_t val)
{
	__asm __volatile("movl %0,%%cr3" : : "r" (val));
}

static gcc_inline uint32_t
rcr3(void)
{
	uint32_t val;
	__asm __volatile("movl %%cr3,%0" : "=r" (val));
	return val;
}

static gcc_inline void
lcr4(uint32_t val)
{
	__asm __volatile("movl %0,%%cr4" : : "r" (val));
}

static gcc_inline uint32_t
rcr4(void)
{
	uint32_t cr4;
	__asm __volatile("movl %%cr4,%0" : "=r" (cr4));
	return cr4;
}

static gcc_inline void
tlbflush(void)
{
	uint32_t cr3;
	__asm __volatile("movl %%cr3,%0" : "=r" (cr3));
	__asm __volatile("movl %0,%%cr3" : : "r" (cr3));
}

static gcc_inline uint32_t
read_eflags(void)
{
	uint32_t eflags;
	__asm __volatile("pushfl; popl %0" : "=rm" (eflags));
	return eflags;
}

static gcc_inline void
write_eflags(uint32_t eflags)
{
	__asm __volatile("pushl %0; popfl" : : "rm" (eflags));
}

static gcc_inline uint32_t
read_ebp(void)
{
	uint32_t ebp;
	__asm __volatile("movl %%ebp,%0" : "=rm" (ebp));
	return ebp;
}

static gcc_inline uint32_t
read_esp(void)
{
	uint32_t esp;
	__asm __volatile("movl %%esp,%0" : "=rm" (esp));
	return esp;
}

static gcc_inline uint16_t
read_cs(void)
{
	uint16_t cs;
	__asm __volatile("movw %%cs,%0" : "=rm" (cs));
	return cs;
}

// Atomically set *addr to newval and return the old value of *addr.
static inline uint32_t
xchg(volatile uint32_t *addr, uint32_t newval)
{
	uint32_t result;

	// The + in "+m" denotes a read-modify-write operand.
	asm volatile("lock; xchgl %0, %1" :
	       "+m" (*addr), "=a" (result) :
	       "1" (newval) :
	       "cc");
	return result;
}

// Atomically add incr to *addr.
static inline void
lockadd(volatile int32_t *addr, int32_t incr)
{
	asm volatile("lock; addl %1,%0" : "+m" (*addr) : "r" (incr) : "cc");
}

// Atomically add incr to *addr and return true if the result is zero.
static inline uint8_t
lockaddz(volatile int32_t *addr, int32_t incr)
{
	uint8_t zero;
	asm volatile("lock; addl %2,%0; setzb %1"
		: "+m" (*addr), "=rm" (zero)
		: "r" (incr)
		: "cc");
	return zero;
}

// Atomically add incr to *addr and return the old value of *addr.
static inline int32_t
xadd(volatile uint32_t *addr, int32_t incr)
{
	int32_t result;

	// The + in "+m" denotes a read-modify-write operand.
	asm volatile("lock; xaddl %0, %1" :
	       "+m" (*addr), "=a" (result) :
	       "1" (incr) :
	       "cc");
	return result;
}

static inline void
halt(void)
{
	asm volatile("hlt");
}

static inline void
pause(void)
{
	asm volatile("pause" : : : "memory");
}

static gcc_inline void
cpuid(uint32_t info, uint32_t *eaxp, uint32_t *ebxp, uint32_t *ecxp, uint32_t *edxp)
{
	uint32_t eax, ebx, ecx, edx;
	asm volatile("cpuid"
		: "=a" (eax), "=b" (ebx), "=c" (ecx), "=d" (edx)
		: "a" (info));
	if (eaxp)
		*eaxp = eax;
	if (ebxp)
		*ebxp = ebx;
	if (ecxp)
		*ecxp = ecx;
	if (edxp)
		*edxp = edx;
}

static inline unsigned int
cpuid_eax(unsigned int op)
{
	unsigned int eax;

	__asm__("pushl %%ebx; cpuid; popl %%ebx"
		: "=a" (eax)
		: "0" (op)
		: "cx", "dx");
	return eax;
}

static inline
unsigned int cpuid_ebx(unsigned int op)
{
	unsigned int eax, ebx;

	//TODO: esi get clobbered?
	__asm__("pushl %%ebx; cpuid; movl %%ebx,%%esi; popl %%ebx"
		: "=a" (eax), "=S" (ebx)
		: "0" (op)
		: "cx", "dx" );
	return ebx;
}

static inline
unsigned int cpuid_ecx(unsigned int op)
{
	unsigned int eax, ecx;

	__asm__("pushl %%ebx; cpuid; popl %%ebx"
		: "=a" (eax), "=c" (ecx)
		: "0" (op)
		: "dx" );
	return ecx;
}

static inline
unsigned int cpuid_edx(unsigned int op)
{
	unsigned int eax, edx;

	__asm__("pushl %%ebx; cpuid; popl %%ebx"
		: "=a" (eax), "=d" (edx)
		: "0" (op)
		: "cx");
	return edx;
}


static gcc_inline uint64_t
rdtsc(void)
{
	uint64_t tsc;
	asm volatile("rdtsc" : "=A" (tsc));
	return tsc;
}

// Enable external device interrupts.
static gcc_inline void
sti(void)
{
	asm volatile("sti");
}

// Disable external device interrupts.
static gcc_inline void
cli(void)
{
	asm volatile("cli");
}

// Byte-swap a 32-bit word to convert to/from big-endian byte order.
// (Reverses the order of the 4 bytes comprising the word.)
static gcc_inline uint32_t
bswap(uint32_t v)
{
	uint32_t r;
	asm volatile("bswap %0" : "=r" (r) : "0" (v));
	return r;
}

// Host/network byte-order conversion for x86
#define htons(v)	(((uint16_t)(v) >> 8) | (uint16_t)((v) << 8))
#define ntohs(v)	(((uint16_t)(v) >> 8) | (uint16_t)((v) << 8))
#define htonl(v)	bswap(v)
#define ntohl(v)	bswap(v)


// General registers in the format pushed by PUSHA instruction.
// We use this instruction to push the general registers only for convenience:
// modern kernels generally avoid it and save the registers manually,
// because that's just as fast or faster and they get to choose
// exactly which registers to save and where.
typedef struct pushregs {
	uint32_t reg_edi;
	uint32_t reg_esi;
	uint32_t reg_ebp;
	uint32_t reg_oesp;		/* Useless */
	uint32_t reg_ebx;
	uint32_t reg_edx;
	uint32_t reg_ecx;
	uint32_t reg_eax;
} pushregs;

// Floating-point/MMX/XMM register save area format,
// in the layout defined by the processor's FXSAVE/FXRSTOR instructions.
typedef gcc_aligned(16) struct fxsave {
	uint16_t fcw;	// byte 0
	uint16_t	fsw;
	uint16_t	ftw;
	uint16_t	fop;
	uint32_t	fpu_ip;
	uint16_t	cs;
	uint16_t	reserved1;
	uint32_t	fpu_dp;			// byte 16
	uint16_t	ds;
	uint16_t	reserved2;
	uint32_t	mxcsr;
	uint32_t	mxcsr_mask;
	uint8_t		st_mm[8][16];		// byte 32: x87/MMX registers
	uint8_t		xmm[8][16];		// byte 160: XMM registers
	uint8_t		reserved3[11][16];	// byte 288: reserved area
	uint8_t		available[3][16];	// byte 464: available to OS
} fxsave;


typedef struct cpu_registers {
    uint32_t reg_eax;
    uint32_t reg_ebx;
    uint32_t reg_ecx;
    uint32_t reg_edx;
    uint32_t reg_ebp;
    uint32_t reg_esp;
    uint32_t reg_esi;
    uint32_t reg_edi;
    uint16_t reg_cs;
    uint16_t reg_cs_pad;
    uint16_t reg_ds;
    uint16_t reg_ds_pad;
    uint16_t reg_es;
    uint16_t reg_es_pad;
    uint16_t reg_gs;
    uint16_t reg_gs_pad;
    uint16_t reg_ss;
    uint16_t reg_ss_pad;
    uint32_t reg_eflags;
    fxsave fx;
} cpu_registers;



//bit operations:
#define ADDR (*(volatile long *) addr)

/**
 * set_bit - Atomically set a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * This function is atomic and may not be reordered.  See __set_bit()
 * if you do not require the atomic guarantees.
 * Note that @nr may be almost arbitrarily large; this function is not
 * restricted to acting on a single-word quantity.
 */
static __inline__ void set_bit(int nr, volatile void * addr)
{
	__asm__ __volatile__(
		"btsl %1,%0"
		:"+m" (ADDR)
		:"dIr" (nr) : "memory");
}


/**
 * clear_bit - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * clear_bit() is atomic and may not be reordered.  However, it does
 * not contain a memory barrier, so if it is used for locking purposes,
 * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()
 * in order to ensure changes are visible on other processors.
 */
static __inline__ void clear_bit(int nr, volatile void * addr)
{
	__asm__ __volatile__(
		"btrl %1,%0"
		:"+m" (ADDR)
		:"dIr" (nr));
}

static __inline__ int constant_test_bit(int nr, const volatile void * addr)
{
	return ((1UL << (nr & 31)) & (((const volatile unsigned int *) addr)[nr >> 5])) != 0;
}

static __inline__ int variable_test_bit(int nr, volatile const void * addr)
{
	int oldbit;

	__asm__ __volatile__(
		"btl %2,%1\n\tsbbl %0,%0"
		:"=r" (oldbit)
		:"m" (ADDR),"dIr" (nr));
	return oldbit;
}

#define test_bit(nr,addr) \
(__builtin_constant_p(nr) ? \
 constant_test_bit((nr),(addr)) : \
 variable_test_bit((nr),(addr)))

#undef ADDR

#endif /* !__ASSEMBLER__ */
#endif /* !_CERTIKOS_BOOT_X86_H_ */
